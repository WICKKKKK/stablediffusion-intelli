Trainer:
  logger: true  # Logger (or iterable collection of loggers) for experiment tracking. A ``True`` value uses the default ``TensorBoardLogger``. ``False`` will disable logging. If multiple loggers are provided and the `save_dir` property of that logger is not set, local files (checkpoints, profiler traces, etc.) are saved in ``default_root_dir`` rather than in the ``log_dir`` of any of the individual loggers.; type: LightningLoggerBase | Iterable | bool
  checkpoint_callback: true # If ``True``, enable checkpointing. It will configure a default ModelCheckpoint callback if there is no user-defined ModelCheckpoint in :paramref:`~pytorch_lightning.trainer.trainer.Trainer.callbacks`.; type: bool
  callbacks: # Add a callback or list of callbacks.; type: List | Callback | NoneType
  default_root_dir: # Default path for logs and weights when no logger/ckpt_callback passed. Default: ``os.getcwd()``. Can be remote file paths such as `s3://mybucket/path` or 'hdfs://path/'; type: str | NoneType
  gradient_clip_val: 0.0 # 0 means don't clip.; type: float
  gradient_clip_algorithm: norm # 'value' means clip_by_value, 'norm' means clip_by_norm. Default: 'norm'; type: str
  process_position: 0 # orders the progress bar when running multiple models on same machine.; type: int
  num_nodes: 1 # number of GPU nodes for distributed training.; type: int
  num_processes: 1 # number of processes for distributed training with distributed_backend="ddp_cpu"; type: int
  devices: # Will be mapped to either `gpus`, `tpu_cores`, `num_processes` or `ipus`, based on the accelerator type.; type: int | str | List | NoneType
  gpus: auto # number of gpus to train on (int) or which GPUs to train on (list or str) applied per node; type: int | str | List | NoneType
  auto_select_gpus: false # If enabled and `gpus` is an integer, pick available gpus automatically. This is especially useful when GPUs are configured to be in "exclusive mode", such that only one process at a time can access them.; type: bool
  tpu_cores: # How many TPU cores to train on (1 or 8) / Single TPU to train on [1]; type: int | str | List | NoneType
  ipus: # How many IPUs to train on.; type: int | NoneType
  log_gpu_memory: # None, 'min_max', 'all'. Might slow performance; type: str | NoneType
  progress_bar_refresh_rate: # How often to refresh progress bar (in steps). Value ``0`` disables progress bar. Ignored when a custom progress bar is passed to :paramref:`~Trainer.callbacks`. Default: None, means a suitable value will be chosen based on the environment (terminal, Google COLAB, etc.).; type: int | NoneType
  overfit_batches: 0.0 # Overfit a fraction of training data (float) or a set number of batches (int).; type: int | float
  track_grad_norm: -1 # -1 no tracking. Otherwise tracks that p-norm. May be set to 'inf' infinity-norm.; type: int | float | str
  check_val_every_n_epoch: 1 # Check val every n train epochs.; type: int
  fast_dev_run: false # runs n if set to ``n`` (int) else 1 if set to ``True`` batch(es) of train, val and test to find any bugs (ie: a sort of unit test).; type: int | bool
  accumulate_grad_batches: 1 # Accumulates grads every k batches or as set up in the dict.; type: int | Dict | List
  max_epochs: # Stop training once this number of epochs is reached. Disabled by default (None). If both max_epochs and max_steps are not specified, defaults to ``max_epochs`` = 1000.; type: int | NoneType
  min_epochs: # Force training for at least these many epochs. Disabled by default (None). If both min_epochs and min_steps are not specified, defaults to ``min_epochs`` = 1.; type: int | NoneType
  max_steps: # Stop training after this number of steps. Disabled by default (None).; type: int | NoneType
  min_steps: # Force training for at least these number of steps. Disabled by default (None).; type: int | NoneType
  max_time: # Stop training after this amount of time has passed. Disabled by default (None). The time duration can be specified in the format DD:HH:MM:SS (days, hours, minutes seconds), as a :class:`datetime.timedelta`, or a dictionary with keys that will be passed to :class:`datetime.timedelta`.; type: str | timedelta | Dict | NoneType
  limit_train_batches: 1.0 # How much of training dataset to check (float = fraction, int = num_batches); type: int | float
  limit_val_batches: 1.0 # How much of validation dataset to check (float = fraction, int = num_batches); type: int | float
  limit_test_batches: 1.0 # How much of test dataset to check (float = fraction, int = num_batches); type: int | float
  limit_predict_batches: 1.0 # How much of prediction dataset to check (float = fraction, int = num_batches); type: int | float
  val_check_interval: 1.0 # How often to check the validation set. Use float to check within a training epoch, use int to check every n steps (batches).; type: int | float
  flush_logs_every_n_steps: 100 # How often to flush logs to disk (defaults to every 100 steps).; type: int
  log_every_n_steps: 50 # How often to log within steps (defaults to every 50 steps).; type: int
  accelerator: # Previously known as distributed_backend (dp, ddp, ddp2, etc...). Can also take in an accelerator object for custom hardware.; type: str | Accelerator | NoneType
  sync_batchnorm: false # Synchronize batch norm layers between process groups/whole world.; type: bool
  precision: 32 # Double precision (64), full precision (32) or half precision (16). Can be used on CPU, GPU or TPUs.; type: int
  weights_summary: top # Prints a summary of the weights when training begins.; type: str | NoneType
  weights_save_path: # Where to save weights if specified. Will override default_root_dir for checkpoints only. Use this if for whatever reason you need the checkpoints stored in a different place than the logs written in `default_root_dir`. Can be remote file paths such as `s3://mybucket/path` or 'hdfs://path/' Defaults to `default_root_dir`.; type: str | NoneType
  num_sanity_val_steps: 2 # Sanity check runs n validation batches before starting the training routine. Set it to `-1` to run all batches in all validation dataloaders.; type: int
  truncated_bptt_steps: # Deprecated in v1.3 to be removed in 1.5. Please use :paramref:`~pytorch_lightning.core.lightning.LightningModule.truncated_bptt_steps` instead.; type: int | NoneType
  resume_from_checkpoint: # Path/URL of the checkpoint from which training is resumed. If there is no checkpoint file at the path, start from scratch. If resuming from mid-epoch checkpoint, training will start from the beginning of the next epoch.; type: str | Path | NoneType
  profiler: # To profile individual steps during training and assist in identifying bottlenecks.; type: BaseProfiler | str | NoneType
  benchmark: True # If true enables cudnn.benchmark.; type: bool
  deterministic: false # If true enables cudnn.deterministic.; type: bool
  reload_dataloaders_every_n_epochs: 0 # Set to a non-negative integer to reload dataloaders every n epochs. Default: 0; type: int
  reload_dataloaders_every_epoch: false # Set to True to reload dataloaders every epoch. .. deprecated:: v1.4 ``reload_dataloaders_every_epoch`` has been deprecated in v1.4 and will be removed in v1.6. Please use ``reload_dataloaders_every_n_epochs``.; type: bool
  auto_lr_find: false # If set to True, will make trainer.tune() run a learning rate finder, trying to optimize initial learning for faster convergence. trainer.tune() method will set the suggested learning rate in self.lr or self.learning_rate in the LightningModule. To use a different key set a string instead of True with the key name.; type: bool | str
  replace_sampler_ddp: true # Explicitly enables or disables sampler replacement. If not specified this will toggled automatically when DDP is used. By default it will add ``shuffle=True`` for train sampler and ``shuffle=False`` for val/test sampler. If you want to customize it, you can set ``replace_sampler_ddp=False`` and add your own distributed sampler.; type: bool
  terminate_on_nan: false # If set to True, will terminate training (by raising a `ValueError`) at the end of each training batch, if any of the parameters or the loss are NaN or +/-inf.; type: bool
  auto_scale_batch_size: false # If set to True, will `initially` run a batch size finder trying to find the largest batch size that fits into memory. The result will be stored in self.batch_size in the LightningModule. Additionally, can be set to either `power` that estimates the batch size through a power search or `binsearch` that estimates the batch size through a binary search.; type: str | bool
  prepare_data_per_node: true # If True, each LOCAL_RANK=0 will call prepare data. Otherwise only NODE_RANK=0, LOCAL_RANK=0 will prepare data; type: bool
  plugins: # Plugins allow modification of core behavior like ddp and amp, and enable custom lightning plugins.; type: List | Plugin | ClusterEnvironment | str | NoneType
  amp_backend: native # The mixed precision backend to use ("native" or "apex"); type: str
  amp_level: O2 # The optimization level to use (O1, O2, etc...).; type: str
  distributed_backend: # deprecated. Please use 'accelerator'; type: str | NoneType
  move_metrics_to_cpu: false # Whether to force internal logged metrics to be moved to cpu. This can save some gpu memory, but can make training slower. Use with attention.; type: bool
  multiple_trainloader_mode: max_size_cycle # How to loop over the datasets when there are multiple train loaders. In 'max_size_cycle' mode, the trainer ends one epoch when the largest dataset is traversed, and smaller datasets reload when running out of their data. In 'min_size' mode, all the datasets reload when reaching the minimum length of datasets.; type: str
  stochastic_weight_avg: false # Whether to use `Stochastic Weight Averaging (SWA) <https://pytorch.org/blog/pytorch-1.6-now-includes-stochastic-weight-averaging/>_`; type: bool
name: ''  # postfix for logdir; type: str
resume: '' # resume from logdir or checkpoint in logdir; type: str
train: true # train; type: bool
no_test: false # disable test; type: bool
project: '' # name of new or path to existing project; type: str
debug: false # enable post-mortem debugging; type: bool
seed: 23 # seed for seed_everything; type: int
postfix: '' # post-postfix for default name; type: str
logdir: logs # directory for logging dat shit; type: str
scale_lr: true # scale base-lr by ngpu * batch_size * n_accumulate; type: bool
model:
  base_learning_rate: 1.0e-06
  target: ldm.models.diffusion.ddpm.LatentDiffusion
  params:
    linear_start: 0.0015
    linear_end: 0.0195
    num_timesteps_cond: 1
    log_every_t: 200
    timesteps: 1000
    first_stage_key: image
    cond_stage_key: class_label
    image_size: 32
    channels: 4
    cond_stage_trainable: true
    conditioning_key: crossattn
    monitor: val/loss_simple_ema
    unet_config:
      target: ldm.modules.diffusionmodules.openaimodel.UNetModel
      params:
        image_size: 32
        in_channels: 4
        out_channels: 4
        model_channels: 256
        attention_resolutions:
        #note: this isn\t actually the resolution but
        # the downsampling factor, i.e. this corresnponds to
        # attention on spatial resolution 8,16,32, as the
        # spatial reolution of the latents is 32 for f8
        - 4
        - 2
        - 1
        num_res_blocks: 2
        channel_mult:
        - 1
        - 2
        - 4
        num_head_channels: 32
        use_spatial_transformer: true
        transformer_depth: 1
        context_dim: 512
    first_stage_config:
      target: ldm.models.autoencoder.VQModelInterface
      params:
        embed_dim: 4
        n_embed: 16384
        ckpt_path: pretrained_models/sd/first_stage/vq-f8/model.ckpt
        ddconfig:
          double_z: false
          z_channels: 4
          resolution: 256
          in_channels: 3
          out_ch: 3
          ch: 128
          ch_mult:
          - 1
          - 2
          - 2
          - 4
          num_res_blocks: 2
          attn_resolutions:
          - 32
          dropout: 0.0
        lossconfig:
          target: torch.nn.Identity
    cond_stage_config:
      target: ldm.modules.encoders.modules.ClassEmbedder
      params:
        embed_dim: 512
        key: class_label
data:
  target: ldm.data.datamodule.DataModuleFromConfig
  params:
    batch_size: 1
    num_workers: 12
    wrap: false
    train:
      target: ldm.data.imagenet.ImageNetTrain
      params:
        config:
          size: 256
    validation:
      target: ldm.data.imagenet.ImageNetValidation
      params:
        config:
          size: 256
lightning:
  callbacks:
    image_logger:
      target: ldm.callbacks.ImageLogger
      params:
        batch_frequency: 5000
        max_images: 8
        increase_log_steps: false